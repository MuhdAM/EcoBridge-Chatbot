<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EcoBridge AI Chatbot</title>
    <link rel="stylesheet" href="static/style.css">
    <link rel="icon" type="image/x-icon" href="{{ url_for('static', filename='favicon.ico') }}">

    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.128/examples/js/loaders/GLTFLoader.js"></script>
    <link rel="stylesheet" href="static/style.css">
</head>
<body>
    <script>
        // Three.js Setup
        let scene, camera, renderer, model, mixer;
        const clock = new THREE.Clock();
        window.talkingAction1 = null; // Make talkingAction1 globally accessible
        window.talkingAction2 = null; // Make talkingAction2 globally accessible
    
        // Helper function to normalize strings
        function normalizeString(str) {
            return str
                .normalize('NFC')
                .replace(/[\u200B-\u200D\uFEFF]/g, '')
                .trim();
        }
    
        // Helper function to log character codes for debugging
        function logCharCodes(str, label) {
            const codes = Array.from(str).map(char => char.charCodeAt(0));
            console.log(`${label} character codes:`, codes);
        }
    
        // Initialize Three.js scene and load the model
        function init() {
            scene = new THREE.Scene();
            camera = new THREE.PerspectiveCamera(50, window.innerWidth / getCanvasHeight(), 0.1, 1000);
            renderer = new THREE.WebGLRenderer({ alpha: true, antialias: true });
            renderer.setSize(window.innerWidth * 0.5, window.innerHeight * 0.7);
            document.body.appendChild(renderer.domElement);
            
            handleResponsiveLayout(); // Initial sizing
            
            // Add light
            const light = new THREE.AmbientLight(0xffffff);
            scene.add(light);
    
            // Load the 3D character (GLB/GLTF)
            const loader = new THREE.GLTFLoader();
            loader.load('/static/models/avatar2.0.glb', function (gltf) {
                model = gltf.scene;
                window.model = model; // Make model globally accessible for morph targets
                scene.add(model);
    
                camera.position.set(0, 1.8, 1);
                camera.lookAt(0, 1.7, 0);
    
                // Load animations
                mixer = new THREE.AnimationMixer(model);
                const clips = gltf.animations;
    
                // Detailed logging of clips
                console.log('Number of animations:', clips.length);
                console.log('Available animations (raw):', clips.map(clip => JSON.stringify(clip.name)));
                console.log('Available animations (trimmed):', clips.map(clip => clip.name.trim()));
    
                // Manual search for "Armature|mixamo.com|Layer0"
                let clip1 = null;
                const targetName1 = "Armature|mixamo.com|Layer0";
                for (let clip of clips) {
                    const normalizedClipName = normalizeString(clip.name);
                    const normalizedTargetName = normalizeString(targetName1);
                    logCharCodes(clip.name, `Clip name (${clip.name})`);
                    logCharCodes(targetName1, `Target name (${targetName1})`);
                    console.log('Comparing normalized clip name:', JSON.stringify(normalizedClipName), 'with target:', JSON.stringify(normalizedTargetName));
                    console.log('Are they equal?', normalizedClipName === normalizedTargetName);
                    if (normalizedClipName === normalizedTargetName) {
                        clip1 = clip;
                        console.log('Found clip "Armature|mixamo.com|Layer0" via manual search.');
                        break;
                    }
                }
    
                if (clip1) {
                    window.talkingAction1 = mixer.clipAction(clip1);
                    window.talkingAction1.setLoop(THREE.LoopRepeat);
                } else {
                    console.warn('Animation clip "Armature|mixamo.com|Layer0" not found via manual search. Falling back to first available animation.');
                    if (clips.length > 0) {
                        clip1 = clips[0];
                        window.talkingAction1 = mixer.clipAction(clip1);
                        window.talkingAction1.setLoop(THREE.LoopRepeat);
                    } else {
                        console.error('No animations available in the model.');
                    }
                }
    
                // Manual search for "Wolf3D_HeadAction"
                let clip2 = null;
                const targetName2 = "Wolf3D_HeadAction";
                for (let clip of clips) {
                    const normalizedClipName = normalizeString(clip.name);
                    const normalizedTargetName = normalizeString(targetName2);
                    logCharCodes(clip.name, `Clip name (${clip.name})`);
                    logCharCodes(targetName2, `Target name (${targetName2})`);
                    console.log('Comparing normalized clip name:', JSON.stringify(normalizedClipName), 'with target:', JSON.stringify(normalizedTargetName));
                    console.log('Are they equal?', normalizedClipName === normalizedTargetName);
                    if (normalizedClipName === normalizedTargetName) {
                        clip2 = clip;
                        console.log('Found clip "Wolf3D_HeadAction" via manual search.');
                        break;
                    }
                }
    
                if (clip2) {
                    window.talkingAction2 = mixer.clipAction(clip2);
                    window.talkingAction2.setLoop(THREE.LoopRepeat);
                } else {
                    console.warn('Animation clip "Wolf3D_HeadAction" not found via manual search. Falling back to second available animation if available.');
                    if (clips.length > 1) {
                        clip2 = clips[1];
                        window.talkingAction2 = mixer.clipAction(clip2);
                        window.talkingAction2.setLoop(THREE.LoopRepeat);
                    }
                }
    
                // Start the animation loop
                window.addEventListener('resize', handleResponsiveLayout);
                animate();
            }, undefined, function (error) {
                console.error('Error loading model:', error);
            });
        }
        
        
       function getCanvasHeight() {
  return window.innerHeight * 0.45; // You can adjust this ratio
}

function handleResponsiveLayout() {
  const width = window.innerWidth;
  const height = getCanvasHeight();

  renderer.setSize(width, height);
  camera.aspect = width / height;
  camera.updateProjectionMatrix();

  const canvasContainer = document.body.appendChild(renderer.domElement);
  canvasContainer.style.height = `${height}px`;
}

    

        function animate() {
            requestAnimationFrame(animate);
            if (mixer) mixer.update(clock.getDelta());
            renderer.render(scene, camera);
        }
    
        // Initialize Three.js
        init();
    </script>

    <div class="chat-container">
        <div class="chat-header">
            <img src="static/ecobridge_logo.png" alt="EcoBridge Logo" class="logo">
            <h2>EcoBridge AI Chatbot</h2>
        </div>
        <div class="chat-box" id="chat-box"></div>
        <div class="chat-input">
            <input type="text" id="user-input" placeholder="Type a message..." autofocus>
            <button onclick="sendMessage()">Send</button>
        </div>
    </div>
    <script src="static/script.js"></script>
</body>
</html>